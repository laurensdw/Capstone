---
title: "Data Science Capstone Assignment 1"
author: "Laurens de Wit"
date: "12th of April 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(plotly))
suppressPackageStartupMessages(library(stringi))
suppressPackageStartupMessages(library(RWeka))
suppressPackageStartupMessages(library(qdap))
suppressPackageStartupMessages(library(tm))
suppressPackageStartupMessages(library(NLP))
suppressPackageStartupMessages(library(wordcloud))
setwd("~/coursera_local/Data")
```

## Loading in Data

First, we'll load in the data:

```{r, warning=FALSE}
# Blogs file
blogs<-file("C:/Users/ldewit/Documents/coursera_local/Data/en_US/en_US.blogs.txt","r")
blogs_lines<-readLines(blogs)
close(blogs)

# News file
news<-file("C:/Users/ldewit/Documents/coursera_local/Data/en_US/en_US.news.txt","r")
news_lines<-readLines(news)
close(news)

# Twitter file
twitter<-file("C:/Users/ldewit/Documents/coursera_local/Data/en_US/en_US.twitter.txt","r")
twitter_lines<-readLines(twitter)
close(twitter)
```

## Summaries of the data

Let's first look at the structure of the lines for each dataset:

Blogs

```{r, echo=FALSE}
summary(nchar(blogs_lines))
```

News

```{r}
summary(nchar(news_lines))
```

Twitter

```{r}
summary(nchar(twitter_lines))
```

## Sampling

Now that we know a couple of details about the structure of the data, it is time to reduce the datasets to samples. This is needed for computational speed as the original datasets are very large.

```{r, echo=TRUE}
set.seed(100)

sample_size = 1000

sample_blog <- blogs_lines[sample(1:length(blogs_lines),sample_size)]
sample_news <- news_lines[sample(1:length(news_lines),sample_size)]
sample_twitter <- twitter_lines[sample(1:length(twitter_lines),sample_size)]
```

Next, we can combine these data sets and remove the large original files from memory:

```{r}
sample_data<-rbind(sample_blog,sample_news,sample_twitter)
rm(blogs_lines,news_lines,twitter_lines)
```

## Cleaning

After obtaining our samples we can start cleaning the data:

```{r}
## Splttings turns of talk into individual setences.
clean_data <- sent_detect(sample_data)
## Setting all characters to lower case.
clean_data <- tolower(clean_data)
## Removing all numbers from the data set.
clean_data <- removeNumbers(clean_data)
## Removing all punctuation.
clean_data <- removePunctuation(clean_data)
## Removing all whitespaces in case there is more than one.
clean_data <- stripWhitespace(clean_data)

# Putting the cleaned data into a df.
clean_data <- data.frame(clean_data, stringsAsFactors=FALSE)
```

## Creation of Document-Term Matrix

```{r}
#Creating the corpus from the sample files
corpus <- Corpus(VectorSource(clean_data))
#Constructing Document-Term Matrix
DTM <- TermDocumentMatrix(corpus, control = list(minWordLength = 1))
#Converting DTM to Matrix
DTM_matrix <- as.matrix(DTM)
#Sorting matrix - words with the highest number of apearances on the top
DTM_ordered <- sort(rowSums(DTM_matrix), decreasing = TRUE)
```

## Visualization

The most common words:

```{r}
head(DTM_ordered)
```

These are all are generally common words. With a wordcloud we can extract words that are generally less common
but have a frequent appearance in the data we are analyzing.

Now displayed in a wordcloud:

```{r, warning=FALSE}
words<-WordTokenizer(clean_data)
wordcloud(words, scale=c(5,0.1), max.words=80, random.order=FALSE)
```

## Conclusions

By running the explorative analysis abvoe we can conclude the following:

- Preprocessing and cleaning the datasets was made possible.
- By cleaning the and merging the datasets in the same way we can come up with a prediction model that covers all information supplied by the different data sets.

